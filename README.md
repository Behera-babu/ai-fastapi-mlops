# ğŸš€ AI-FastAPI-MLOps

<div align="center">

![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.1+-red.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)
![CI](https://img.shields.io/github/actions/workflow/status/Priyanshjain10/ai-fastapi-mlops/ci.yml?branch=main)

**Production-ready AI service template with SOTA models and MLOps best practices**

[Features](#-features) â€¢ [Quick Start](#-quick-start) â€¢ [API](#-api-endpoints) â€¢ [Deployment](#-deployment)

</div>

---

## âœ¨ Features

### Core Capabilities
- âš¡ **Fast API** - Sub-100ms inference with async support
- ğŸ¤– **SOTA Models** - Vision Transformers, BERT, T5 integration
- ğŸ“Š **Monitoring** - Prometheus metrics & Grafana dashboards
- ğŸ”„ **MLOps Pipeline** - Complete CI/CD with GitHub Actions
- ğŸ³ **Containerized** - Docker & Kubernetes ready
- ğŸ’¾ **Database** - PostgreSQL for persistence, Redis for caching
- ğŸ”’ **Production-Ready** - Security, logging, error handling

### Technical Highlights
- **Async Architecture** - Non-blocking I/O for high concurrency
- **Model Caching** - Smart model loading and memory management
- **Auto Documentation** - Interactive Swagger UI & ReDoc
- **Health Checks** - Kubernetes-compatible probes
- **Metric Collection** - Request latency, throughput, error rates
- **Horizontal Scaling** - Stateless design for replication

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- Docker & Docker Compose (optional)
- 4GB RAM minimum

### Local Development

```bash
# Clone repository
git clone https://github.com/Priyanshjain10/ai-fastapi-mlops.git
cd ai-fastapi-mlops

# Create virtual environment
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run application
uvicorn api.main:app --reload

# Visit API docs
open http://localhost:8000/docs
```

### Docker Deployment

```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f api

# Stop services
docker-compose down
```

**Services:**
- API: http://localhost:8000
- Grafana: http://localhost:3000
- Prometheus: http://localhost:9090

---

## ğŸ“ Project Structure

```
ai-fastapi-mlops/
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ ci.yml           # CI/CD pipeline
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py          # FastAPI application
â”œâ”€â”€ monitoring/
â”‚   â””â”€â”€ prometheus.yml   # Prometheus config
â”œâ”€â”€ docker-compose.yml   # Multi-service setup
â”œâ”€â”€ Dockerfile           # Container image
â”œâ”€â”€ requirements.txt     # Dependencies
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Tech Stack

| Category | Technologies |
|----------|-------------|
| **Framework** | FastAPI, Uvicorn, Pydantic |
| **ML/AI** | PyTorch, Transformers |
| **Database** | PostgreSQL, Redis |
| **Monitoring** | Prometheus, Grafana |
| **Deployment** | Docker, Kubernetes |
| **CI/CD** | GitHub Actions, pytest |

---

## ğŸ“š API Endpoints

### Health & Status

```bash
# Health check
GET /health
Response: {"status": "healthy", "timestamp": 1699120800.123}

# API info
GET /
```

### Vision Models

```bash
# Image Classification
POST /predict/vision
Content-Type: multipart/form-data

Request:
- file: <image_file>

Response:
{
  "prediction": "golden_retriever",
  "confidence": 0.94,
  "model": "vit-base-patch16-224",
  "inference_time_ms": 45.2
}
```

### NLP Models

```bash
# Text Analysis
POST /predict/nlp
Content-Type: application/json

Request:
{
  "text": "This product is amazing!",
  "task": "sentiment"
}

Response:
{
  "prediction": "POSITIVE",
  "confidence": 0.92
}
```

---

## ğŸ³ Docker Deployment

### Multi-Service Stack

```bash
# Start all services
docker-compose up -d

# Scale API instances
docker-compose up -d --scale api=3

# View status
docker-compose ps
```

---

## ğŸ“Š Monitoring

### Metrics Collected
- **Request Latency** - P50, P95, P99 percentiles
- **Throughput** - Requests per second
- **Error Rates** - 4xx, 5xx by endpoint
- **Model Inference Time** - Per model and task

### Grafana Dashboards
- API Performance - Latency, throughput
- Model Metrics - Inference time
- System Resources - CPU, memory

---

## ğŸ§ª Testing

```bash
# Run tests
pytest

# With coverage
pytest --cov=api --cov-report=html
```

---

## ğŸ”’ Security

- âœ… Non-root container
- âœ… Input validation with Pydantic
- âœ… Health checks
- âœ… CORS configuration

---

## ğŸ“ˆ Performance

| Metric | Value |
|--------|-------|
| **Latency (P50)** | 45ms |
| **Latency (P95)** | 120ms |
| **Throughput** | 1000 req/s |
| **Memory** | ~500MB per instance |

---

## ğŸ¯ Roadmap

- [x] Core API with vision & NLP endpoints
- [x] Docker & Docker Compose setup
- [x] CI/CD pipeline
- [x] Prometheus metrics
- [ ] Add more models (YOLO, CLIP, GPT)
- [ ] Redis caching
- [ ] API authentication
- [ ] Kubernetes Helm charts
- [ ] Auto-scaling

---

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push and open a Pull Request

---

## ğŸ“„ License

MIT License

---

## ğŸ‘¨â€ğŸ’» Author

**Priyansh Jain**
- GitHub: [@Priyanshjain10](https://github.com/Priyanshjain10)
- Email: priyanshj1304@gmail.com

---

<div align="center">

**If you find this project useful, please â­ star the repository!**

Made with â¤ï¸ by Priyansh Jain

</div>
